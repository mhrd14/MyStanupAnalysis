{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason for me to taking up this project/mission was my obvious interest and love for stand up comedy and also because i wanted to know why exactly do i love hasan minhaj's content. What is it that he does so differently from others? \n",
    "\n",
    "Anyway, we are about to find that out! As i was going the process of building this project and making a workflow, i realised how complex this could turn out to be. My main questions i wanted to be answered were...\n",
    "1. How the comedians differ from each other? [This is observed from the fulldata dataframe's analysis, as it contains transcripts of 16 comedians] \n",
    "2. How each comedians' content differ between their own stand up specials [This analysis will be addressed by amy schumer, john mulaney and kevin hart's transcripts from various specials]\n",
    "3. What is the style of hasan minhaj and how does he express it? [From the data scraped from his 22 episodes from his Netflix Special called Patriot Act]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will have to release the analysis in phases as it is massive and in the first phase, i am putting out the data extraction and intense cleaning process in two different notebooks for ease of use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been scraped from two websites \n",
    "1. Scraps From The Loft (http://scrapsfromtheloft.com) \n",
    "2. yousubtitles (https://www.yousubtitles.com/Patriot-Act-cd-149803/) \n",
    "The second website provides subtitles, transcripts for videos on youtube and thankfully hasan releases his episodes on youtube in addition to Netflix which enabled me to get the transcripts manually and store them in a folder called hasantrans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes transcript data from scrapsfromtheloft.com\n",
    "def url_to_transcript(url):\n",
    "    '''Returns transcript data specifically from scrapsfromtheloft.com.'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find(class_=\"post-content\").find_all('p')]\n",
    "    return text\n",
    "\n",
    "# URLs of transcripts in scope\n",
    "urls = ['https://scrapsfromtheloft.com/2019/12/05/tiffany-haddish-black-mitzvah-transcript/',\n",
    "        'https://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
    "        'https://scrapsfromtheloft.com/2018/05/05/john-mulaney-kid-gorgeous-at-radio-city-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/10/21/hasan-minhaj-homecoming-king-2017-full-transcript/',\n",
    "        'https://scrapsfromtheloft.com/2018/05/15/ali-wong-hard-knock-wife-full-transcript/',\n",
    "       'https://scrapsfromtheloft.com/2020/03/01/pete-davidson-alive-from-new-york-transcript/',\n",
    "       'https://scrapsfromtheloft.com/2019/04/05/kevin-hart-irresponsible-transcript/',\n",
    "       'https://scrapsfromtheloft.com/2019/03/23/amy-schumer-growing-transcript/',\n",
    "       'https://scrapsfromtheloft.com/2019/02/17/ken-jeong-you-complete-me-ho-transcript/',\n",
    "       'https://scrapsfromtheloft.com/2018/12/22/ellen-degeneres-relatable-transcript/',\n",
    "       'https://scrapsfromtheloft.com/2018/12/15/vir-das-losing-it-transcript/',\n",
    "       'https://scrapsfromtheloft.com/2018/11/21/trevor-noah-son-of-patricia-transcript/',\n",
    "       'https://scrapsfromtheloft.com/2018/10/26/adam-sandler-100-fresh-transcript/',\n",
    "       'https://scrapsfromtheloft.com/2020/05/10/russell-peters-deported-transcript/',\n",
    "        'https://scrapsfromtheloft.com/2020/06/01/kenny-sebastian-dont-be-that-guy-transcript/',\n",
    "        'https://scrapsfromtheloft.com/2017/08/30/aziz-ansari-intimate-moments-sensual-evening-2010-full-transcript/']\n",
    "\n",
    "# Comedian names \n",
    "comedians = ['Tiffany Haddish', 'Ricky Gervais', 'John Mulaney', 'Hasan Minhaj', 'Ali Wong', 'Pete Davidson',\n",
    "          'Kevin Hart', 'Amy Schumer','Ken Jeong', 'Ellen Degeneres','Vir Das','Trevor Noah','Adam Sandler',\n",
    "          'Russell Peters','Kenny Sebastian','Aziz Ansari']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To study the variation in comic delivery over years\n",
    "\n",
    "kevinurls = ['https://scrapsfromtheloft.com/2019/08/08/kevin-hart-gun-compartment-transcript/',\n",
    "             'https://scrapsfromtheloft.com/2019/04/05/kevin-hart-irresponsible-transcript/',\n",
    "            'https://scrapsfromtheloft.com/2018/01/14/kevin-hart-what-now-2016-full-transcript/',\n",
    "            'https://scrapsfromtheloft.com/2018/01/14/kevin-hart-let-me-explain-2013-full-transcript/',\n",
    "            'https://scrapsfromtheloft.com/2017/09/02/kevin-hart-seriously-funny-2010-full-transcript/' ]\n",
    "kevinindex = ['2020','2019','2016','2013','2010']\n",
    "\n",
    "amyurls = ['https://scrapsfromtheloft.com/2017/09/08/amy-schumer-mostly-sex-stuff-2012-full-transcript/',\n",
    "          'https://scrapsfromtheloft.com/2017/09/05/amy-schumer-live-apollo-2015-full-transcript/',\n",
    "          'https://scrapsfromtheloft.com/2017/06/20/amy-schumer-leather-special-2017-full-transcript/',\n",
    "          'https://scrapsfromtheloft.com/2019/03/23/amy-schumer-growing-transcript/']\n",
    "amyindex = ['2012','2015','2017','2019']\n",
    "\n",
    "johnurls = ['https://scrapsfromtheloft.com/2018/09/23/john-mulaney-snl-monologue-2018-transcript/',\n",
    "           'https://scrapsfromtheloft.com/2018/05/05/john-mulaney-kid-gorgeous-at-radio-city-full-transcript/',\n",
    "           'https://scrapsfromtheloft.com/2017/09/25/john-mulaney-new-in-town-2012-full-transcript/',\n",
    "           'https://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/']\n",
    "johnindex = ['2019','2018','2012','2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To analyse the variations and also the content and any other latent features and simply because i like \n",
    "# Hasan Minhaj Transcripts - Patriot Act From the URL: https://www.yousubtitles.com/Patriot-Act-cd-149803/\n",
    "# The text files have been dowloaded and saved into a folder called hasantrans \n",
    "\n",
    "directory = 'hasantrans'\n",
    "episodes = []\n",
    "texts = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = open(\"hasantrans/\"+ filename,\"r\",encoding=\"utf8\")\n",
    "    lines = f.readlines()\n",
    "    epname = filename.split('-Patriot')[0].replace('-',' ')\n",
    "    episodes.append(epname)\n",
    "    actext = ' '.join(lines)\n",
    "    texts.append(actext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16  transcripts extracted\n",
      "5  transcripts extracted\n",
      "4  transcripts extracted\n",
      "4  transcripts extracted\n"
     ]
    }
   ],
   "source": [
    "# Loading everyones' transcripts into a list called transcripts and doing the same for kevin, amy and john\n",
    "transcripts = [url_to_transcript(u) for u in urls]\n",
    "print(len(transcripts),' transcripts extracted')\n",
    "kevintranscripts = [url_to_transcript(u) for u in kevinurls]\n",
    "print(len(kevintranscripts),' transcripts extracted')\n",
    "amytranscripts = [url_to_transcript(u) for u in amyurls]\n",
    "print(len(amytranscripts),' transcripts extracted')\n",
    "johntranscripts = [url_to_transcript(u) for u in johnurls]\n",
    "print(len(johntranscripts),' transcripts extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making separate dataframes for the comedians as they are to serve different purposes. Hence we have 5 dataframes in the end. \n",
    "# One having all the comedians, to compare and analyse. Other 4 corresponding to kevin, amy, john and hasan to be analysed\n",
    "# individually \n",
    "\n",
    "datadict = {}\n",
    "for i, c in enumerate(comedians):\n",
    "    datadict[c] = ' '.join(transcripts[i])\n",
    "\n",
    "kevindatadict = {}\n",
    "for i, c in enumerate(kevinindex):\n",
    "    kevindatadict[c] = ' '.join(kevintranscripts[i])\n",
    "\n",
    "amydatadict = {}\n",
    "for i, c in enumerate(amyindex):\n",
    "    amydatadict[c] = ' '.join(amytranscripts[i])\n",
    "\n",
    "johndatadict = {}\n",
    "for i, c in enumerate(johnindex):\n",
    "    johndatadict[c] = ' '.join(johntranscripts[i])\n",
    "\n",
    "hasandata = pd.DataFrame({'episode':episodes,'transcript': texts},index = episodes) #My favorite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is made ready for cleaning and analysis\n",
    "\n",
    "fulldata = pd.DataFrame.from_dict(datadict,orient = 'index', columns = ['transcript'])\n",
    "\n",
    "kevindata = pd.DataFrame.from_dict(kevindatadict,orient = 'index', columns = ['transcript']) #extreme swear words usage\n",
    "\n",
    "amydata = pd.DataFrame.from_dict(amydatadict,orient = 'index', columns = ['transcript']) #Complete positive script \n",
    "\n",
    "johndata = pd.DataFrame.from_dict(johndatadict,orient = 'index', columns = ['transcript']) #moderate and balanced script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the raw dataframes \n",
    "\n",
    "fulldata.to_pickle(\"rawfulldata.pkl\")\n",
    "kevindata.to_pickle(\"rawkevindata.pkl\")\n",
    "amydata.to_pickle(\"rawamydata.pkl\")\n",
    "johndata.to_pickle(\"rawjohndata.pkl\")\n",
    "hasandata.to_pickle(\"rawhasandata.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
